---
layout: post
title: Mutual Information
date: 2020-02-20
---
Mutual information is a metric used extensively across many fields. Wikipedia gives the following intuitive definition:

> The Mutual Information of two random variables is a measure of the mutual dependence between the two variables.

For two discrete random variables, it can be calculated as:

$$ \operatorname{I}(X;Y) = \sum_{y \in \mathcal Y} \sum_{x \in \mathcal X}
    { p_{(X,Y)}(x,y) \log{ \left(\frac{p_{(X,Y)}(x,y)}{p_X(x)\,p_Y(y)} \right) }} $$

where $$ p_{(X,Y)} $$ is the joint distribution of $$X$$ and $$Y$$, and $$p_X$$ and $$p_Y$$ are the marginal probability mass functions of $$X$$ and $$Y$$ respectively [^1]

This definition seems easy enough to apply. However, one thing which one quickly realizes when using this metric is that calculating the probability mass functions requires picking a bin size! One might notice this and hope that there is a quick and natural choice, or that bin size doesn't change the results too much, but unfortunately, neither of those things are the case!

# Matlab Implementation
Following the above definition, I wrote some code to calculate the mutual information for some simple variables:



# KNN Approach


# Applications

A 2018 paper which I personally think is fascinating makes the claim that:

> Framing word embedding as *metric recovery* of a semantic space unifies existing word embedding algorithms, ties them to manifold learning, and demonstrates that existing algorithms are consistent metric recovery methods given co-occurrence counts from random walks. [^2]

The connection between metric recovery and mutual information is that a seminal paper demonstrated that human word ratings are linearly related to many distributionaly derived pointwise mutual information (PMI) [^3]. Following this motivation, the paper mentions how in recent times, it has been shown that many current distributional models are related to eachother through PMI. To quote the paper:

> In terms of algorithms, Levy and Goldberg (2014b) demonstrated that the global minimum of the skip-gram method with negative sampling of Mikolov et al. (2013b) implicitly factorizes a shifted version of the PMI matrix of word-context pairs. 

I mention these results here because I think that with the prevalence of interesting results using mutual information in so many fields, we should be aware of the nuance in actually calculating its value! 

# References
[^1]: <https://en.wikipedia.org/wiki/Mutual_information#In_terms_of_PMFs_for_discrete_distributions> 
[^2]: Hashimoto, T. B., Alvarez-Melis, D., & Jaakkola, T. S. (2018). Word Embeddings as Metric Recovery in Semantic Spaces. Transactions of the Association for Computational Linguistics, 4, 273–286. doi: 10.1162/tacl_a_00098
[^3]: Kenneth Ward Church andPatrick Hanks. 1990. Word association norms, mutual information, and lexicography.Comput. Linguist.,16(1):22–29