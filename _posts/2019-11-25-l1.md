---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: post
title: L1 Norm
date: 2019-11-25
permalink: /posts/l1
---
# The L1 norm

We will consider the simplest case of univariate linear regression. In this case:
\[ c=\lambda \]

$$ \hat{y}= f_w(\textbf{x})=w_n x_n $$

To find the best estimates of w, we can use the L2 norm of the loss function, for which we get a new function of \( w \), namely, \( Loss(h_\textbf{w} \(
This has a unique standard solution which can be found in the usual way. For linear functions, this is always **convex**. In practice, we might be concerned with overfiting, and thus need regularization. We thus update the cost function with a regularization term

$$ Cost(f)= EmpLoss(f_w(x)) + \lambda Complexity(w) $$

A general family of regularization functions is just the norm of the coefficients!
This is where L1 regularization is beneficial. We can get a feel for this by considering a an upper limit for complexity \( Complexity(w) \leq c \(
This means that we are trying to minimize the cost within the regular *unit circle of the norm*. In general, L_2 minumums are equally likely to occur anywhere on the edge of the circle, but more likly to occur on the corner of the box!

![picture](/assets/l1.png)

