---
layout: post
title: Mutual Information
date: 2020-02-20
---
Mutual information is a metric used extensively across many fields. 

# Intuition
First, prior to talking definitions and nuance, we can get a general sense for what we are trying to measure. Given two sets of observations, we might wonder whether the observations are independent. One indication that they may not be independent is if they are correlated. However, correlation only captures a linear dependence. A more general sense of what is sought is whether knowing the value of observation, provides any information about the value of the second random variable.

This intuition tells us that for two independent variables, the joint distribution of two variables should simply be the product of the two marginal distributions. Assume for two random variables **A** and **B** and observations $$a$$ and $$b$$

$$ P_{AB}(a,b) = P_A(a) * P_B(b) $$

Equivalently

$$ P_{AB}(a | b) = P_A(a) $$

This means that two variables are dependent it the above is not the case! In general, our goal is to come up with a method of generating a number (preferably between 0 and 1) which tells us how far away from completely independent two distributions are. Since entropy is designed to be additive between two independent distributions, a metric might *feel* something along the lines of the following::

$$ I(A,B) = \frac{H(P_{AB})}{H(P_A) + H(P_B)} $$

Which might read something along the lines of "The information gain equals the entropy of the joint distribution over the sum of entropies of the marginal distributions". In this case a value of 1 indicates independence, and values closer two zero indicate more dependence.

# Formal Definition

Wikipedia gives the following definition:

> The Mutual Information of two random variables is a measure of the mutual dependence between the two variables.

For two discrete random variables, it can be calculated as:

$$ \operatorname{I}(X;Y) = \sum_{y \in \mathcal Y} \sum_{x \in \mathcal X}
    { p_{(X,Y)}(x,y) \log{ \left(\frac{p_{(X,Y)}(x,y)}{p_X(x)\,p_Y(y)} \right) }} $$

where $$ p_{(X,Y)} $$ is the joint distribution of $$X$$ and $$Y$$, and $$p_X$$ and $$p_Y$$ are the marginal probability mass functions of $$X$$ and $$Y$$ respectively [^1]

This definition seems easy enough to apply. However, one thing which one quickly realizes when using this metric is that calculating the probability mass(or density) functions requires picking a bin size (or kernel smoothing parameter)! One might notice this and hope that there is a quick and natural choice, or that bin size doesn't change the results too much, but unfortunately, neither of those things are the case!

## Matlab Implementation
Following the above definition, I wrote some code to calculate the mutual information for two random variables (A and B):

```matlab
n=6; % # of bins for making PMF
g1 = histcounts(A,n); % Bin the data
g2 = histcounts(B,n);
g3 = histcounts2(A,B,n);
pmf1 = repmat(g1/sum(g1),n,1); % change counts to %, and make grid for sum
pmf2 = repmat((g2/sum(g2))',1,n);
pmf3 = g3/sum(g3(:));
pmf1(pmf3==0)=[]; pmf2(pmf3==0)=[]; pmf3(pmf3==0)=[]; % remove 0 entries
MI = sum(pmf3.*log(pmf3./(pmf1.*pmf2))) % Calculate MI score
```
Note, I naively choose to uniform number and spacing of bins. From here we can generate some test data and see the results!

# Circle

# Bin Spacing Modification
It is important to note that to give our algorithm the best chance of success, I use non-uniform bin size with bins chosen from the joint distribution. We can look at the dependence on the bin size



## KNN Approach


## Applications

A 2018 paper which I personally think is fascinating makes the claim that:

> Framing word embedding as *metric recovery* of a semantic space unifies existing word embedding algorithms, ties them to manifold learning, and demonstrates that existing algorithms are consistent metric recovery methods given co-occurrence counts from random walks. [^2]

The connection between metric recovery and mutual information is that a seminal paper demonstrated that human word ratings are linearly related to many distributionaly derived pointwise mutual information (PMI) [^3]. Following this motivation, the paper mentions how in recent times, it has been shown that many current distributional models are related to eachother through PMI. To quote the paper:

> In terms of algorithms, Levy and Goldberg (2014b) demonstrated that the global minimum of the skip-gram method with negative sampling of Mikolov et al. (2013b) implicitly factorizes a shifted version of the PMI matrix of word-context pairs. 

I mention these results here because I think that with the prevalence of interesting results using mutual information in so many fields, we should be aware of the nuance in actually calculating its value! 

# References
[^1]: <https://en.wikipedia.org/wiki/Mutual_information#In_terms_of_PMFs_for_discrete_distributions> 
[^2]: Hashimoto, T. B., Alvarez-Melis, D., & Jaakkola, T. S. (2018). Word Embeddings as Metric Recovery in Semantic Spaces. Transactions of the Association for Computational Linguistics, 4, 273–286. doi: 10.1162/tacl_a_00098
[^3]: Kenneth Ward Church andPatrick Hanks. 1990. Word association norms, mutual information, and lexicography.Comput. Linguist.,16(1):22–29